{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: The Closed-Form Solution\n",
    "\n",
    "This notebook demonstrates the ordinary least squares (OLS) closed-form solution for linear regression, connecting the matrix formulation in Chapter 2 to concrete computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlone_theme as mt\n",
    "\n",
    "mt.set_notebook_mode()\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Normal Equations\n",
    "\n",
    "For a linear model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, the OLS estimator minimizes the residual sum of squares:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = \\arg\\min_{\\boldsymbol{\\beta}} \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2$$\n",
    "\n",
    "Taking the derivative and setting it to zero yields the **normal equations**:\n",
    "\n",
    "$$\\mathbf{X}^\\top \\mathbf{X} \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "When $\\mathbf{X}^\\top \\mathbf{X}$ is invertible, we get the closed-form solution:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_closed_form(X, y):\n",
    "    \"\"\"Compute OLS coefficients using the normal equations.\"\"\"\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "    beta = np.linalg.inv(XtX) @ Xty\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Small Worked Example\n",
    "\n",
    "Consider a simple dataset with 5 observations. We'll compute everything step-by-step to see the matrix operations in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dataset: house size (hundreds of sq ft) vs price (thousands of $)\n",
    "x = np.array([8, 10, 12, 14, 16])     # Feature\n",
    "y = np.array([150, 200, 250, 280, 310])  # Target\n",
    "\n",
    "print(\"Feature (x):  \", x)\n",
    "print(\"Target  (y):  \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the design matrix with intercept column\n",
    "X = np.column_stack([np.ones(len(x)), x])\n",
    "print(\"Design matrix X:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design matrix has a column of ones for the intercept term. Now let's compute each component of the normal equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute X^T X\n",
    "XtX = X.T @ X\n",
    "print(\"X^T X:\")\n",
    "print(XtX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute X^T y\n",
    "Xty = X.T @ y\n",
    "print(\"X^T y:\")\n",
    "print(Xty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute (X^T X)^{-1}\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "print(\"(X^T X)^{-1}:\")\n",
    "print(XtX_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute beta = (X^T X)^{-1} X^T y\n",
    "beta = XtX_inv @ Xty\n",
    "print(f\"Estimated coefficients: beta_0 = {beta[0]:.2f}, beta_1 = {beta[1]:.2f}\")\n",
    "print(f\"\\nInterpretation: Price = {beta[0]:.0f} + {beta[1]:.1f} × Size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = X @ beta\n",
    "residuals = y - y_pred\n",
    "\n",
    "print(\"Actual:     \", y)\n",
    "print(\"Predicted:  \", y_pred.round(1))\n",
    "print(\"Residuals:  \", residuals.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Data points\n",
    "mt.scatter_with_border(ax, x, y, color=mt.GREEN, size=80)\n",
    "\n",
    "# Regression line\n",
    "x_line = np.linspace(6, 18, 100)\n",
    "y_line = beta[0] + beta[1] * x_line\n",
    "ax.plot(x_line, y_line, color=mt.GREEN, linewidth=2, label='OLS fit')\n",
    "\n",
    "# Show residuals as vertical lines\n",
    "for xi, yi, ypi in zip(x, y, y_pred):\n",
    "    ax.plot([xi, xi], [yi, ypi], color=mt.RED, linewidth=1.5, linestyle='--', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('House Size (hundreds of sq ft)')\n",
    "ax.set_ylabel('Price (thousands of $)')\n",
    "mt.apply_economist_style(ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed red lines show the residuals—the vertical distances that OLS minimizes in aggregate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Geometric View: Projection\n",
    "\n",
    "The OLS solution projects the target vector $\\mathbf{y}$ onto the column space of $\\mathbf{X}$. The fitted values $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$ are the closest point to $\\mathbf{y}$ in this subspace.\n",
    "\n",
    "The **hat matrix** $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top$ performs this projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hat matrix\n",
    "H = X @ np.linalg.inv(X.T @ X) @ X.T\n",
    "print(\"Hat matrix H (5×5):\")\n",
    "print(H.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: H @ y gives fitted values\n",
    "y_hat_via_H = H @ y\n",
    "print(\"Fitted values via H @ y:  \", y_hat_via_H.round(1))\n",
    "print(\"Fitted values via X @ β:  \", y_pred.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connecting to scikit-learn\n",
    "\n",
    "Of course, in practice we use library implementations. Let's verify that sklearn's `LinearRegression` gives the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x.reshape(-1, 1), y)  # sklearn expects 2D input\n",
    "\n",
    "print(f\"sklearn intercept: {model.intercept_:.2f}\")\n",
    "print(f\"sklearn slope:     {model.coef_[0]:.2f}\")\n",
    "print(f\"\\nOur intercept:     {beta[0]:.2f}\")\n",
    "print(f\"Our slope:         {beta[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multiple Regression\n",
    "\n",
    "The same formulation extends to multiple features. Let's add a second feature: number of bedrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended dataset with two features\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "\n",
    "size = np.random.uniform(8, 20, n)       # House size\n",
    "bedrooms = np.random.randint(2, 5, n)    # Number of bedrooms\n",
    "\n",
    "# True relationship: Price = 20 + 15*size + 25*bedrooms + noise\n",
    "price = 20 + 15 * size + 25 * bedrooms + np.random.normal(0, 15, n)\n",
    "\n",
    "print(f\"Generated {n} observations with 2 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design matrix with intercept\n",
    "X_multi = np.column_stack([np.ones(n), size, bedrooms])\n",
    "print(f\"Design matrix shape: {X_multi.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(X_multi[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS solution\n",
    "beta_multi = ols_closed_form(X_multi, price)\n",
    "\n",
    "print(\"Estimated coefficients:\")\n",
    "print(f\"  Intercept:       {beta_multi[0]:.2f} (true: 20)\")\n",
    "print(f\"  Size effect:     {beta_multi[1]:.2f} (true: 15)\")\n",
    "print(f\"  Bedroom effect:  {beta_multi[2]:.2f} (true: 25)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with sklearn\n",
    "X_sklearn = np.column_stack([size, bedrooms])\n",
    "model_multi = LinearRegression().fit(X_sklearn, price)\n",
    "\n",
    "print(\"\\nsklearn coefficients:\")\n",
    "print(f\"  Intercept:       {model_multi.intercept_:.2f}\")\n",
    "print(f\"  Size effect:     {model_multi.coef_[0]:.2f}\")\n",
    "print(f\"  Bedroom effect:  {model_multi.coef_[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Residual Analysis\n",
    "\n",
    "Examining residuals helps verify model assumptions: they should be roughly normally distributed with constant variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals for multiple regression\n",
    "y_pred_multi = X_multi @ beta_multi\n",
    "residuals_multi = price - y_pred_multi\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "ax = axes[0]\n",
    "mt.scatter_with_border(ax, y_pred_multi, price, color=mt.GREEN, size=40)\n",
    "ax.plot([100, 350], [100, 350], 'k--', alpha=0.5, linewidth=1)\n",
    "ax.set_xlabel('Predicted Price')\n",
    "ax.set_ylabel('Actual Price')\n",
    "mt.apply_economist_style(ax)\n",
    "ax.set_title('Predicted vs Actual', fontsize=12, loc='left')\n",
    "\n",
    "# 2. Residuals vs Fitted\n",
    "ax = axes[1]\n",
    "mt.scatter_with_border(ax, y_pred_multi, residuals_multi, color=mt.GREEN, size=40)\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax.set_xlabel('Fitted Values')\n",
    "ax.set_ylabel('Residuals')\n",
    "mt.apply_economist_style(ax)\n",
    "ax.set_title('Residuals vs Fitted', fontsize=12, loc='left')\n",
    "\n",
    "# 3. Histogram of residuals\n",
    "ax = axes[2]\n",
    "ax.hist(residuals_multi, bins=15, color=mt.GREEN, edgecolor='white', alpha=0.8)\n",
    "ax.set_xlabel('Residual')\n",
    "ax.set_ylabel('Count')\n",
    "mt.apply_economist_style(ax)\n",
    "ax.set_title('Residual Distribution', fontsize=12, loc='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals show no obvious pattern against fitted values (good—constant variance) and are roughly symmetric around zero (good—no systematic bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Numerical Stability: Using `lstsq`\n",
    "\n",
    "Computing $(\\mathbf{X}^\\top\\mathbf{X})^{-1}$ directly can be numerically unstable for ill-conditioned matrices. NumPy's `lstsq` uses the more stable SVD-based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using numpy's lstsq (more numerically stable)\n",
    "beta_lstsq, residuals_lstsq, rank, singular_values = np.linalg.lstsq(X_multi, price, rcond=None)\n",
    "\n",
    "print(\"Coefficients via lstsq:\")\n",
    "print(f\"  Intercept:       {beta_lstsq[0]:.4f}\")\n",
    "print(f\"  Size effect:     {beta_lstsq[1]:.4f}\")\n",
    "print(f\"  Bedroom effect:  {beta_lstsq[2]:.4f}\")\n",
    "\n",
    "print(f\"\\nDifference from closed-form: {np.max(np.abs(beta_lstsq - beta_multi)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The closed-form OLS solution $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$ provides:\n",
    "\n",
    "- **Direct computation** of optimal coefficients (no iteration)\n",
    "- **Geometric interpretation** as projection onto column space of $\\mathbf{X}$\n",
    "- **Exact solution** (up to numerical precision)\n",
    "\n",
    "However, the closed-form approach has limitations:\n",
    "- Requires $\\mathbf{X}^\\top\\mathbf{X}$ to be invertible\n",
    "- Computationally expensive for large $n$ (matrix inversion is $O(p^3)$)\n",
    "- Cannot easily incorporate regularization\n",
    "\n",
    "These limitations motivate **gradient descent**, which we explore in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
