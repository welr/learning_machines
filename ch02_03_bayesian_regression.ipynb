{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "**Note**: This notebook covers optional enrichment material for readers with interest in Bayesian methods. It requires PyMC, which is not part of the core dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "Frequentist regression gives us point estimates: a single \"best\" value for each coefficient. Bayesian regression gives us **distributions**—complete descriptions of our uncertainty about each parameter. This notebook makes the Bayesian approach concrete: we specify priors, watch them update to posteriors as data arrives, and see how prior beliefs and data jointly determine our conclusions.\n",
    "\n",
    "**What computation adds**: Bayesian inference involves integrals that rarely have closed forms. MCMC sampling lets us approximate posteriors for any model. Visualization of prior→posterior transformation shows exactly how data changes beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import our custom theme\n",
    "import mlone_theme as mt\n",
    "plt.style.use('mlone_style.mplstyle')\n",
    "mt.set_notebook_mode()\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for PyMC installation\n",
    "try:\n",
    "    import pymc as pm\n",
    "    import arviz as az\n",
    "    print(f\"PyMC version: {pm.__version__}\")\n",
    "    PYMC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyMC not installed. Install with: pip install pymc arviz\")\n",
    "    print(\"This notebook will show analytical solutions but skip MCMC sections.\")\n",
    "    PYMC_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Frequentist vs. Bayesian: Two Philosophies\n",
    "\n",
    "| Aspect | Frequentist | Bayesian |\n",
    "|--------|-------------|----------|\n",
    "| Parameters | Fixed but unknown | Random variables with distributions |\n",
    "| Data | Random (from repeated sampling) | Fixed (what we observed) |\n",
    "| Inference | Point estimates + confidence intervals | Posterior distributions |\n",
    "| Prior knowledge | Not formally incorporated | Encoded in prior distributions |\n",
    "\n",
    "**Bayes' theorem** connects prior beliefs to posterior beliefs:\n",
    "\n",
    "$$P(\\theta | \\text{data}) = \\frac{P(\\text{data} | \\theta) \\cdot P(\\theta)}{P(\\text{data})}$$\n",
    "\n",
    "$$\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. A Simple Example: Estimating a Mean\n",
    "\n",
    "Before tackling regression, let's see how Bayesian updating works for a simple problem: estimating the mean of a normal distribution.\n",
    "\n",
    "**Setup:**\n",
    "- Data: $x_i \\sim N(\\mu, \\sigma^2)$ with known $\\sigma^2 = 1$\n",
    "- Prior: $\\mu \\sim N(\\mu_0, \\sigma_0^2)$\n",
    "- Posterior: $\\mu | \\text{data} \\sim N(\\mu_n, \\sigma_n^2)$\n",
    "\n",
    "The posterior has a closed form (conjugate prior):\n",
    "\n",
    "$$\\mu_n = \\frac{\\sigma^2 \\mu_0 + n \\sigma_0^2 \\bar{x}}{\\sigma^2 + n \\sigma_0^2}, \\quad \\sigma_n^2 = \\frac{\\sigma^2 \\sigma_0^2}{\\sigma^2 + n \\sigma_0^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_normal_update(data, prior_mean, prior_var, likelihood_var):\n",
    "    \"\"\"\n",
    "    Analytical Bayesian update for normal mean with known variance.\n",
    "    \n",
    "    Returns posterior mean and variance.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    data_mean = np.mean(data) if n > 0 else 0\n",
    "    \n",
    "    # Posterior parameters\n",
    "    posterior_var = 1 / (1/prior_var + n/likelihood_var)\n",
    "    posterior_mean = posterior_var * (prior_mean/prior_var + n*data_mean/likelihood_var)\n",
    "    \n",
    "    return posterior_mean, posterior_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters\n",
    "true_mean = 2.5\n",
    "known_var = 1.0\n",
    "\n",
    "# Prior: centered at 0, fairly uncertain\n",
    "prior_mean = 0.0\n",
    "prior_var = 4.0\n",
    "\n",
    "# Generate data sequentially\n",
    "np.random.seed(42)\n",
    "all_data = np.random.normal(true_mean, np.sqrt(known_var), 50)\n",
    "\n",
    "# Visualize prior → posterior updating\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sample_sizes = [0, 1, 3, 10, 25, 50]\n",
    "x_range = np.linspace(-4, 6, 200)\n",
    "\n",
    "for ax, n in zip(axes, sample_sizes):\n",
    "    data = all_data[:n] if n > 0 else []\n",
    "    \n",
    "    post_mean, post_var = bayesian_normal_update(data, prior_mean, prior_var, known_var)\n",
    "    post_std = np.sqrt(post_var)\n",
    "    \n",
    "    # Prior\n",
    "    prior_pdf = stats.norm.pdf(x_range, prior_mean, np.sqrt(prior_var))\n",
    "    ax.fill_between(x_range, prior_pdf, alpha=0.3, color=mt.GRAY, label='Prior')\n",
    "    \n",
    "    # Posterior\n",
    "    post_pdf = stats.norm.pdf(x_range, post_mean, post_std)\n",
    "    ax.fill_between(x_range, post_pdf, alpha=0.5, color=mt.GREEN, label='Posterior')\n",
    "    ax.plot(x_range, post_pdf, color=mt.GREEN, linewidth=2)\n",
    "    \n",
    "    # True value\n",
    "    ax.axvline(true_mean, color=mt.RED, linestyle='--', linewidth=2, label=f'True μ = {true_mean}')\n",
    "    \n",
    "    # Data points\n",
    "    if n > 0:\n",
    "        ax.scatter(data, np.zeros(n) - 0.02, color=mt.BLUE, s=30, alpha=0.7, zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('μ')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'n = {n} observations\\nPosterior: {post_mean:.2f} ± {1.96*post_std:.2f}',\n",
    "                 fontweight='bold')\n",
    "    ax.set_xlim(-4, 6)\n",
    "    ax.set_ylim(-0.05, 1.2)\n",
    "    \n",
    "    if n == 0:\n",
    "        ax.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    mt.apply_economist_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observations:**\n",
    "- With **no data** (n=0), the posterior equals the prior\n",
    "- As **data accumulates**, the posterior concentrates around the true value\n",
    "- The posterior is a **compromise** between prior and data\n",
    "- With enough data, the prior becomes irrelevant (the data dominates)\n",
    "\n",
    "---\n",
    "## 3. Bayesian Linear Regression: The Model\n",
    "\n",
    "Now let's apply the Bayesian approach to linear regression.\n",
    "\n",
    "**Model:**\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "**Priors:**\n",
    "- $\\beta_0 \\sim N(0, \\tau_0^2)$\n",
    "- $\\beta_1 \\sim N(0, \\tau_1^2)$\n",
    "- $\\sigma \\sim \\text{HalfNormal}(s)$ (must be positive)\n",
    "\n",
    "**Goal:** Compute the posterior $P(\\beta_0, \\beta_1, \\sigma | \\text{data})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression data\n",
    "np.random.seed(42)\n",
    "\n",
    "true_beta0 = 1.0\n",
    "true_beta1 = 2.0\n",
    "true_sigma = 0.5\n",
    "\n",
    "n_samples = 30\n",
    "X = np.random.uniform(0, 3, n_samples)\n",
    "y = true_beta0 + true_beta1 * X + np.random.normal(0, true_sigma, n_samples)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.scatter(X, y, c=mt.GREEN, s=60, alpha=0.7, edgecolors='white', linewidths=1, \n",
    "           label='Observations')\n",
    "\n",
    "x_line = np.linspace(0, 3, 100)\n",
    "ax.plot(x_line, true_beta0 + true_beta1 * x_line, c=mt.GRAY, linestyle='--', \n",
    "        linewidth=2, label=f'True line: y = {true_beta0} + {true_beta1}x')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Regression Data', fontweight='bold', loc='left')\n",
    "ax.legend()\n",
    "mt.apply_economist_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Analytical Solution (Conjugate Prior)\n",
    "\n",
    "For linear regression with Gaussian priors on coefficients and known variance, the posterior is also Gaussian. Let's compute it analytically first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_linear_regression_analytical(X, y, prior_mean, prior_cov, noise_var):\n",
    "    \"\"\"\n",
    "    Analytical Bayesian linear regression with Gaussian prior.\n",
    "    \n",
    "    Parameters:\n",
    "        X: Design matrix (n, p) including intercept column\n",
    "        y: Target values (n,)\n",
    "        prior_mean: Prior mean for coefficients (p,)\n",
    "        prior_cov: Prior covariance matrix (p, p)\n",
    "        noise_var: Known noise variance σ²\n",
    "    \n",
    "    Returns:\n",
    "        posterior_mean: (p,)\n",
    "        posterior_cov: (p, p)\n",
    "    \"\"\"\n",
    "    prior_precision = np.linalg.inv(prior_cov)\n",
    "    \n",
    "    # Posterior precision and covariance\n",
    "    posterior_precision = prior_precision + (1/noise_var) * X.T @ X\n",
    "    posterior_cov = np.linalg.inv(posterior_precision)\n",
    "    \n",
    "    # Posterior mean\n",
    "    posterior_mean = posterior_cov @ (prior_precision @ prior_mean + (1/noise_var) * X.T @ y)\n",
    "    \n",
    "    return posterior_mean, posterior_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create design matrix with intercept\n",
    "X_design = np.column_stack([np.ones(n_samples), X])\n",
    "\n",
    "# Prior: fairly vague\n",
    "prior_mean = np.array([0.0, 0.0])\n",
    "prior_cov = np.array([[10.0, 0.0],\n",
    "                      [0.0, 10.0]])\n",
    "\n",
    "# Assume known noise variance (for analytical solution)\n",
    "noise_var = true_sigma**2\n",
    "\n",
    "# Compute posterior\n",
    "post_mean, post_cov = bayesian_linear_regression_analytical(\n",
    "    X_design, y, prior_mean, prior_cov, noise_var\n",
    ")\n",
    "\n",
    "print(\"Analytical Bayesian Regression Results:\")\n",
    "print(f\"  Posterior mean β₀: {post_mean[0]:.3f} (true: {true_beta0})\")\n",
    "print(f\"  Posterior mean β₁: {post_mean[1]:.3f} (true: {true_beta1})\")\n",
    "print(f\"  Posterior std β₀:  {np.sqrt(post_cov[0,0]):.3f}\")\n",
    "print(f\"  Posterior std β₁:  {np.sqrt(post_cov[1,1]):.3f}\")\n",
    "\n",
    "# Compare to OLS\n",
    "ols_beta = np.linalg.lstsq(X_design, y, rcond=None)[0]\n",
    "print(f\"\\nOLS estimates:\")\n",
    "print(f\"  β₀: {ols_beta[0]:.3f}\")\n",
    "print(f\"  β₁: {ols_beta[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Visualizing the Posterior Distribution\n",
    "\n",
    "Unlike OLS which gives point estimates, Bayesian regression gives a **joint distribution** over all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Marginal posterior for β₀\n",
    "ax = axes[0]\n",
    "b0_range = np.linspace(post_mean[0] - 3*np.sqrt(post_cov[0,0]),\n",
    "                       post_mean[0] + 3*np.sqrt(post_cov[0,0]), 100)\n",
    "b0_pdf = stats.norm.pdf(b0_range, post_mean[0], np.sqrt(post_cov[0,0]))\n",
    "ax.fill_between(b0_range, b0_pdf, alpha=0.5, color=mt.GREEN)\n",
    "ax.plot(b0_range, b0_pdf, color=mt.GREEN, linewidth=2)\n",
    "ax.axvline(true_beta0, color=mt.RED, linestyle='--', linewidth=2, label=f'True β₀ = {true_beta0}')\n",
    "ax.axvline(post_mean[0], color=mt.BLUE, linestyle='-', linewidth=2, label=f'Posterior mean')\n",
    "ax.set_xlabel('β₀ (intercept)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Posterior: Intercept', fontweight='bold', loc='left')\n",
    "ax.legend(fontsize=9)\n",
    "mt.apply_economist_style(ax)\n",
    "\n",
    "# Marginal posterior for β₁\n",
    "ax = axes[1]\n",
    "b1_range = np.linspace(post_mean[1] - 3*np.sqrt(post_cov[1,1]),\n",
    "                       post_mean[1] + 3*np.sqrt(post_cov[1,1]), 100)\n",
    "b1_pdf = stats.norm.pdf(b1_range, post_mean[1], np.sqrt(post_cov[1,1]))\n",
    "ax.fill_between(b1_range, b1_pdf, alpha=0.5, color=mt.GREEN)\n",
    "ax.plot(b1_range, b1_pdf, color=mt.GREEN, linewidth=2)\n",
    "ax.axvline(true_beta1, color=mt.RED, linestyle='--', linewidth=2, label=f'True β₁ = {true_beta1}')\n",
    "ax.axvline(post_mean[1], color=mt.BLUE, linestyle='-', linewidth=2, label=f'Posterior mean')\n",
    "ax.set_xlabel('β₁ (slope)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Posterior: Slope', fontweight='bold', loc='left')\n",
    "ax.legend(fontsize=9)\n",
    "mt.apply_economist_style(ax)\n",
    "\n",
    "# Joint posterior contours\n",
    "ax = axes[2]\n",
    "b0_grid = np.linspace(post_mean[0] - 3*np.sqrt(post_cov[0,0]),\n",
    "                      post_mean[0] + 3*np.sqrt(post_cov[0,0]), 100)\n",
    "b1_grid = np.linspace(post_mean[1] - 3*np.sqrt(post_cov[1,1]),\n",
    "                      post_mean[1] + 3*np.sqrt(post_cov[1,1]), 100)\n",
    "B0, B1 = np.meshgrid(b0_grid, b1_grid)\n",
    "pos = np.dstack((B0, B1))\n",
    "rv = stats.multivariate_normal(post_mean, post_cov)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "ax.contourf(B0, B1, Z, levels=20, cmap='Greens', alpha=0.7)\n",
    "ax.contour(B0, B1, Z, levels=5, colors='darkgreen', linewidths=0.5)\n",
    "ax.scatter(true_beta0, true_beta1, c=mt.RED, s=100, marker='*', \n",
    "           zorder=5, label='True values', edgecolors='white', linewidths=1)\n",
    "ax.scatter(post_mean[0], post_mean[1], c=mt.BLUE, s=100, marker='o',\n",
    "           zorder=5, label='Posterior mean', edgecolors='white', linewidths=1)\n",
    "ax.set_xlabel('β₀')\n",
    "ax.set_ylabel('β₁')\n",
    "ax.set_title('Joint Posterior', fontweight='bold', loc='left')\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Posterior Predictive Distribution\n",
    "\n",
    "For a new input $x^*$, the **posterior predictive distribution** accounts for:\n",
    "1. Uncertainty about the parameters (posterior variance)\n",
    "2. Observation noise (σ²)\n",
    "\n",
    "$$p(y^* | x^*, \\text{data}) = \\int p(y^* | x^*, \\theta) p(\\theta | \\text{data}) d\\theta$$\n",
    "\n",
    "This gives us **credible intervals** for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_predictive(x_new, post_mean, post_cov, noise_var):\n",
    "    \"\"\"\n",
    "    Compute posterior predictive mean and variance for new inputs.\n",
    "    \n",
    "    x_new: New input points (m,)\n",
    "    Returns: predictive mean (m,), predictive variance (m,)\n",
    "    \"\"\"\n",
    "    X_new = np.column_stack([np.ones(len(x_new)), x_new])\n",
    "    \n",
    "    # Predictive mean: E[y*] = X* @ posterior_mean\n",
    "    pred_mean = X_new @ post_mean\n",
    "    \n",
    "    # Predictive variance: Var[y*] = X* @ posterior_cov @ X*.T + noise_var\n",
    "    pred_var = np.sum(X_new @ post_cov * X_new, axis=1) + noise_var\n",
    "    \n",
    "    return pred_mean, pred_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "x_plot = np.linspace(-0.5, 3.5, 100)\n",
    "pred_mean, pred_var = posterior_predictive(x_plot, post_mean, post_cov, noise_var)\n",
    "pred_std = np.sqrt(pred_var)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# 95% credible interval\n",
    "ax.fill_between(x_plot, pred_mean - 1.96*pred_std, pred_mean + 1.96*pred_std,\n",
    "                alpha=0.3, color=mt.GREEN, label='95% credible interval')\n",
    "\n",
    "# Posterior mean prediction\n",
    "ax.plot(x_plot, pred_mean, c=mt.GREEN, linewidth=2.5, label='Posterior mean')\n",
    "\n",
    "# True line\n",
    "ax.plot(x_plot, true_beta0 + true_beta1 * x_plot, c=mt.GRAY, linestyle='--',\n",
    "        linewidth=2, label='True line')\n",
    "\n",
    "# Data\n",
    "ax.scatter(X, y, c=mt.BLUE, s=60, alpha=0.7, edgecolors='white', linewidths=1,\n",
    "           label='Observations', zorder=5)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Bayesian Regression: Posterior Predictive Distribution', \n",
    "             fontweight='bold', loc='left')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(-0.5, 3.5)\n",
    "mt.apply_economist_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The credible interval is **wider** away from the data center—where we have less information, we're more uncertain. This is a natural consequence of the Bayesian approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Sampling from the Posterior\n",
    "\n",
    "We can draw **regression lines** from the posterior to visualize uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample coefficients from posterior\n",
    "n_samples_draw = 100\n",
    "beta_samples = np.random.multivariate_normal(post_mean, post_cov, n_samples_draw)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Posterior samples in coefficient space\n",
    "ax = axes[0]\n",
    "ax.scatter(beta_samples[:, 0], beta_samples[:, 1], c=mt.GREEN, s=30, alpha=0.5,\n",
    "           edgecolors='none', label='Posterior samples')\n",
    "ax.scatter(true_beta0, true_beta1, c=mt.RED, s=150, marker='*',\n",
    "           zorder=5, label='True values', edgecolors='white', linewidths=1)\n",
    "ax.scatter(post_mean[0], post_mean[1], c=mt.BLUE, s=100, marker='o',\n",
    "           zorder=5, label='Posterior mean', edgecolors='white', linewidths=1)\n",
    "\n",
    "# Add posterior ellipse\n",
    "from matplotlib.patches import Ellipse\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(post_cov)\n",
    "angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "for n_std in [1, 2]:\n",
    "    ell = Ellipse(xy=post_mean, width=2*n_std*np.sqrt(eigenvalues[0]),\n",
    "                  height=2*n_std*np.sqrt(eigenvalues[1]), angle=angle,\n",
    "                  facecolor='none', edgecolor=mt.GREEN, linewidth=1.5,\n",
    "                  linestyle='--' if n_std == 2 else '-')\n",
    "    ax.add_patch(ell)\n",
    "\n",
    "ax.set_xlabel('β₀')\n",
    "ax.set_ylabel('β₁')\n",
    "ax.set_title('Posterior Samples in Coefficient Space', fontweight='bold', loc='left')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_aspect('equal', adjustable='datalim')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('black')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Corresponding regression lines\n",
    "ax = axes[1]\n",
    "for i in range(min(50, n_samples_draw)):\n",
    "    y_sample = beta_samples[i, 0] + beta_samples[i, 1] * x_plot\n",
    "    ax.plot(x_plot, y_sample, c=mt.GREEN, alpha=0.15, linewidth=1)\n",
    "\n",
    "# Posterior mean\n",
    "ax.plot(x_plot, pred_mean, c=mt.GREEN, linewidth=2.5, label='Posterior mean')\n",
    "\n",
    "# True line\n",
    "ax.plot(x_plot, true_beta0 + true_beta1 * x_plot, c=mt.RED, linestyle='--',\n",
    "        linewidth=2, label='True line')\n",
    "\n",
    "# Data\n",
    "ax.scatter(X, y, c=mt.BLUE, s=60, alpha=0.7, edgecolors='white', linewidths=1,\n",
    "           zorder=5)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Posterior Sample Regression Lines', fontweight='bold', loc='left')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(-0.5, 3.5)\n",
    "mt.apply_economist_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. The Effect of the Prior\n",
    "\n",
    "Let's see how different priors affect the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "priors = [\n",
    "    (\"Vague prior\", np.array([0, 0]), np.array([[100, 0], [0, 100]])),\n",
    "    (\"Informative (correct)\", np.array([1, 2]), np.array([[0.5, 0], [0, 0.5]])),\n",
    "    (\"Informative (wrong)\", np.array([5, -1]), np.array([[0.5, 0], [0, 0.5]])),\n",
    "]\n",
    "\n",
    "for ax, (title, p_mean, p_cov) in zip(axes, priors):\n",
    "    pm, pc = bayesian_linear_regression_analytical(X_design, y, p_mean, p_cov, noise_var)\n",
    "    pred_m, pred_v = posterior_predictive(x_plot, pm, pc, noise_var)\n",
    "    pred_s = np.sqrt(pred_v)\n",
    "    \n",
    "    # Credible interval\n",
    "    ax.fill_between(x_plot, pred_m - 1.96*pred_s, pred_m + 1.96*pred_s,\n",
    "                    alpha=0.3, color=mt.GREEN)\n",
    "    ax.plot(x_plot, pred_m, c=mt.GREEN, linewidth=2.5, label='Posterior mean')\n",
    "    \n",
    "    # True and data\n",
    "    ax.plot(x_plot, true_beta0 + true_beta1 * x_plot, c=mt.GRAY, linestyle='--',\n",
    "            linewidth=2, label='True line')\n",
    "    ax.scatter(X, y, c=mt.BLUE, s=40, alpha=0.7, edgecolors='white', linewidths=0.5)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'{title}\\nβ₀={pm[0]:.2f}, β₁={pm[1]:.2f}', fontweight='bold')\n",
    "    ax.set_xlim(-0.5, 3.5)\n",
    "    ax.set_ylim(-2, 10)\n",
    "    if ax == axes[0]:\n",
    "        ax.legend(loc='upper left', fontsize=9)\n",
    "    mt.apply_economist_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- **Vague prior**: Posterior dominated by data (similar to OLS)\n",
    "- **Correct informative prior**: Tighter credible intervals, faster convergence\n",
    "- **Wrong informative prior**: Posterior is pulled toward wrong values, but still improves with more data\n",
    "\n",
    "---\n",
    "## 9. Connection to Ridge Regression\n",
    "\n",
    "A Gaussian prior on coefficients is equivalent to **L2 regularization** (Ridge regression)!\n",
    "\n",
    "Ridge minimizes:\n",
    "$$\\sum_i (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\|\\boldsymbol{\\beta}\\|^2$$\n",
    "\n",
    "This is the **MAP estimate** (maximum a posteriori) under a Gaussian prior with variance $\\sigma^2/\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Compare Ridge to Bayesian posterior mean\n",
    "lambda_values = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "print(\"Ridge vs Bayesian MAP comparison:\")\n",
    "print(f\"{'λ':>8} {'Ridge β₀':>12} {'Ridge β₁':>12} {'Bayes β₀':>12} {'Bayes β₁':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for lam in lambda_values:\n",
    "    # Ridge regression\n",
    "    ridge = Ridge(alpha=lam, fit_intercept=True)\n",
    "    ridge.fit(X.reshape(-1, 1), y)\n",
    "    ridge_b0, ridge_b1 = ridge.intercept_, ridge.coef_[0]\n",
    "    \n",
    "    # Bayesian with corresponding prior variance\n",
    "    prior_var = noise_var / lam\n",
    "    p_cov = np.array([[100, 0], [0, prior_var]])  # Don't regularize intercept much\n",
    "    pm, _ = bayesian_linear_regression_analytical(X_design, y, np.array([0, 0]), p_cov, noise_var)\n",
    "    \n",
    "    print(f\"{lam:>8.2f} {ridge_b0:>12.4f} {ridge_b1:>12.4f} {pm[0]:>12.4f} {pm[1]:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. MCMC with PyMC (Optional)\n",
    "\n",
    "For models without closed-form posteriors, we use **Markov Chain Monte Carlo (MCMC)** sampling. PyMC makes this straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYMC_AVAILABLE:\n",
    "    # Build PyMC model\n",
    "    with pm.Model() as regression_model:\n",
    "        # Priors\n",
    "        beta0 = pm.Normal('beta0', mu=0, sigma=10)\n",
    "        beta1 = pm.Normal('beta1', mu=0, sigma=10)\n",
    "        sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "        \n",
    "        # Linear model\n",
    "        mu = beta0 + beta1 * X\n",
    "        \n",
    "        # Likelihood\n",
    "        y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)\n",
    "        \n",
    "        # Sample from posterior\n",
    "        trace = pm.sample(2000, tune=1000, cores=1, random_seed=42,\n",
    "                          progressbar=True, return_inferencedata=True)\n",
    "    \n",
    "    print(\"\\nPyMC Posterior Summary:\")\n",
    "    print(az.summary(trace, var_names=['beta0', 'beta1', 'sigma']))\n",
    "else:\n",
    "    print(\"PyMC not available. Skipping MCMC section.\")\n",
    "    print(\"Install with: pip install pymc arviz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYMC_AVAILABLE:\n",
    "    # Visualize posterior distributions\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    \n",
    "    for ax, var, true_val, label in zip(\n",
    "        axes, \n",
    "        ['beta0', 'beta1', 'sigma'],\n",
    "        [true_beta0, true_beta1, true_sigma],\n",
    "        ['β₀ (intercept)', 'β₁ (slope)', 'σ (noise)']\n",
    "    ):\n",
    "        samples = trace.posterior[var].values.flatten()\n",
    "        \n",
    "        ax.hist(samples, bins=50, density=True, alpha=0.7, color=mt.GREEN,\n",
    "                edgecolor='white', linewidth=0.5)\n",
    "        ax.axvline(true_val, color=mt.RED, linestyle='--', linewidth=2,\n",
    "                   label=f'True = {true_val}')\n",
    "        ax.axvline(samples.mean(), color=mt.BLUE, linestyle='-', linewidth=2,\n",
    "                   label=f'Mean = {samples.mean():.3f}')\n",
    "        \n",
    "        # 95% credible interval\n",
    "        ci_low, ci_high = np.percentile(samples, [2.5, 97.5])\n",
    "        ax.axvspan(ci_low, ci_high, alpha=0.2, color=mt.GREEN)\n",
    "        \n",
    "        ax.set_xlabel(label)\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(f'Posterior: {label}', fontweight='bold', loc='left')\n",
    "        ax.legend(fontsize=9)\n",
    "        mt.apply_economist_style(ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYMC_AVAILABLE:\n",
    "    # Posterior predictive with MCMC samples\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Draw regression lines from posterior samples\n",
    "    b0_samples = trace.posterior['beta0'].values.flatten()\n",
    "    b1_samples = trace.posterior['beta1'].values.flatten()\n",
    "    \n",
    "    for i in range(0, len(b0_samples), 50):  # Every 50th sample\n",
    "        y_line = b0_samples[i] + b1_samples[i] * x_plot\n",
    "        ax.plot(x_plot, y_line, c=mt.GREEN, alpha=0.05, linewidth=1)\n",
    "    \n",
    "    # Posterior mean\n",
    "    y_mean = b0_samples.mean() + b1_samples.mean() * x_plot\n",
    "    ax.plot(x_plot, y_mean, c=mt.GREEN, linewidth=2.5, label='Posterior mean')\n",
    "    \n",
    "    # True line\n",
    "    ax.plot(x_plot, true_beta0 + true_beta1 * x_plot, c=mt.GRAY, linestyle='--',\n",
    "            linewidth=2, label='True line')\n",
    "    \n",
    "    # Data\n",
    "    ax.scatter(X, y, c=mt.BLUE, s=60, alpha=0.7, edgecolors='white', linewidths=1,\n",
    "               zorder=5, label='Observations')\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('MCMC Posterior Predictive', fontweight='bold', loc='left')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_xlim(-0.5, 3.5)\n",
    "    mt.apply_economist_style(ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**Bayesian vs. Frequentist:**\n",
    "\n",
    "| Aspect | Frequentist (OLS) | Bayesian |\n",
    "|--------|-------------------|----------|\n",
    "| Output | Point estimates | Full posterior distributions |\n",
    "| Uncertainty | Confidence intervals (frequentist interpretation) | Credible intervals (probability statements) |\n",
    "| Prior knowledge | Not incorporated | Formally specified |\n",
    "| Small samples | Can be unstable | Regularized by prior |\n",
    "\n",
    "**Key concepts:**\n",
    "- **Prior**: Our beliefs before seeing data\n",
    "- **Likelihood**: How probable is the data given parameters?\n",
    "- **Posterior**: Updated beliefs after seeing data\n",
    "- **Posterior predictive**: Distribution over future observations\n",
    "\n",
    "**Practical insights:**\n",
    "- With vague priors and enough data, Bayesian and frequentist results converge\n",
    "- Gaussian priors ↔ Ridge regularization\n",
    "- MCMC enables Bayesian inference for complex models\n",
    "\n",
    "**When to use Bayesian methods:**\n",
    "- When you have meaningful prior information\n",
    "- When you need uncertainty quantification\n",
    "- When sample sizes are small\n",
    "- For hierarchical/multilevel models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploration\n",
    "\n",
    "Try modifying:\n",
    "- **Sample size**: How does the posterior change with n = 5, 50, 500?\n",
    "- **Prior strength**: What happens with very tight priors?\n",
    "- **Model complexity**: Extend to polynomial regression with PyMC\n",
    "- **Hierarchical models**: Add group-level variation with PyMC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
