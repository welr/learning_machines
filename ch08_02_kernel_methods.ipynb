{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods\n",
    "\n",
    "Some patterns are not linearly separable in their original representation but become separable in a higher-dimensional space. The **kernel trick** lets us work in that space implicitly, without ever computing the transformation. This notebook makes kernels concrete: we see linear classifiers fail, watch kernels succeed, and visualize how the choice of kernel shapes the decision boundary.\n",
    "\n",
    "**What computation adds**: Equations define kernel functions; here we see what they *do*—how an RBF kernel bends space to wrap around clusters, how polynomial kernels create curved boundaries, and what happens when kernel parameters are poorly chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the notebook directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import our custom theme\n",
    "import mlone_theme as mt\n",
    "\n",
    "# Apply the style and set notebook mode (green palette)\n",
    "plt.style.use('mlone_style.mplstyle')\n",
    "mt.set_notebook_mode()\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Problem: Non-Linear Decision Boundaries\n",
    "\n",
    "Many real datasets are not linearly separable. Let's create a classic example: the **XOR problem** and a **circular** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_circles_data(n_samples=200, noise=0.1, factor=0.5, seed=42):\n",
    "    \"\"\"Generate two concentric circles.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n_per_class = n_samples // 2\n",
    "    \n",
    "    # Outer circle (class 0)\n",
    "    theta_outer = np.random.uniform(0, 2*np.pi, n_per_class)\n",
    "    r_outer = 1.0 + np.random.normal(0, noise, n_per_class)\n",
    "    X_outer = np.column_stack([r_outer * np.cos(theta_outer), \n",
    "                                r_outer * np.sin(theta_outer)])\n",
    "    \n",
    "    # Inner circle (class 1)\n",
    "    theta_inner = np.random.uniform(0, 2*np.pi, n_per_class)\n",
    "    r_inner = factor + np.random.normal(0, noise, n_per_class)\n",
    "    X_inner = np.column_stack([r_inner * np.cos(theta_inner), \n",
    "                                r_inner * np.sin(theta_inner)])\n",
    "    \n",
    "    X = np.vstack([X_outer, X_inner])\n",
    "    y = np.array([0]*n_per_class + [1]*n_per_class)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def make_xor_data(n_samples=200, noise=0.3, seed=42):\n",
    "    \"\"\"Generate XOR-like data.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n_per_quadrant = n_samples // 4\n",
    "    \n",
    "    # Class 0: top-left and bottom-right\n",
    "    X0_tl = np.random.randn(n_per_quadrant, 2) * noise + [-1, 1]\n",
    "    X0_br = np.random.randn(n_per_quadrant, 2) * noise + [1, -1]\n",
    "    \n",
    "    # Class 1: top-right and bottom-left\n",
    "    X1_tr = np.random.randn(n_per_quadrant, 2) * noise + [1, 1]\n",
    "    X1_bl = np.random.randn(n_per_quadrant, 2) * noise + [-1, -1]\n",
    "    \n",
    "    X = np.vstack([X0_tl, X0_br, X1_tr, X1_bl])\n",
    "    y = np.array([0]*n_per_quadrant*2 + [1]*n_per_quadrant*2)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate both datasets\n",
    "X_circles, y_circles = make_circles_data(300, noise=0.1, factor=0.4)\n",
    "X_xor, y_xor = make_xor_data(300, noise=0.35)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Circles\n",
    "ax = axes[0]\n",
    "ax.scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], \n",
    "           c=mt.BLUE, s=50, alpha=0.7, edgecolors='white', linewidths=0.5, label='Class 0')\n",
    "ax.scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], \n",
    "           c=mt.RED, s=50, alpha=0.7, edgecolors='white', linewidths=0.5, label='Class 1')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_title('Concentric Circles', fontweight='bold', loc='left')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_aspect('equal')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('black')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# XOR\n",
    "ax = axes[1]\n",
    "ax.scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], \n",
    "           c=mt.BLUE, s=50, alpha=0.7, edgecolors='white', linewidths=0.5, label='Class 0')\n",
    "ax.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], \n",
    "           c=mt.RED, s=50, alpha=0.7, edgecolors='white', linewidths=0.5, label='Class 1')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_title('XOR Problem', fontweight='bold', loc='left')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_aspect('equal')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('black')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"No straight line can separate either dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Linear Classifiers Fail\n",
    "\n",
    "Let's confirm that a linear classifier (logistic regression) struggles with these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_decision_boundary(ax, model, X, y, title):\n",
    "    \"\"\"Plot decision boundary for a classifier.\"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap([mt.BLUE, mt.RED]))\n",
    "    ax.contour(xx, yy, Z, colors='black', linewidths=1, alpha=0.5)\n",
    "    \n",
    "    ax.scatter(X[y==0, 0], X[y==0, 1], c=mt.BLUE, s=40, alpha=0.7, \n",
    "               edgecolors='white', linewidths=0.5)\n",
    "    ax.scatter(X[y==1, 0], X[y==1, 1], c=mt.RED, s=40, alpha=0.7,\n",
    "               edgecolors='white', linewidths=0.5)\n",
    "    \n",
    "    acc = model.score(X, y)\n",
    "    ax.set_title(f'{title}\\nAccuracy: {acc:.1%}', fontweight='bold', loc='left')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Linear classifier on circles\n",
    "lr_circles = LogisticRegression()\n",
    "lr_circles.fit(X_circles, y_circles)\n",
    "plot_decision_boundary(axes[0], lr_circles, X_circles, y_circles, 'Linear: Circles')\n",
    "\n",
    "# Linear classifier on XOR\n",
    "lr_xor = LogisticRegression()\n",
    "lr_xor.fit(X_xor, y_xor)\n",
    "plot_decision_boundary(axes[1], lr_xor, X_xor, y_xor, 'Linear: XOR')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Linear classifiers achieve ~50% accuracy—no better than random guessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. The Feature Space Idea\n",
    "\n",
    "What if we transform the data into a space where it *is* linearly separable?\n",
    "\n",
    "For the circles data, consider the transformation $\\phi(x_1, x_2) = x_1^2 + x_2^2$ (the squared distance from the origin). In this 1D space, the classes separate perfectly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform circles data\n",
    "r_squared = X_circles[:, 0]**2 + X_circles[:, 1]**2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Original space\n",
    "ax = axes[0]\n",
    "ax.scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], \n",
    "           c=mt.BLUE, s=50, alpha=0.7, edgecolors='white', linewidths=0.5, label='Class 0')\n",
    "ax.scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], \n",
    "           c=mt.RED, s=50, alpha=0.7, edgecolors='white', linewidths=0.5, label='Class 1')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_title('Original Space: Not Separable', fontweight='bold', loc='left')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('black')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Transformed space\n",
    "ax = axes[1]\n",
    "ax.scatter(r_squared[y_circles==0], np.zeros(sum(y_circles==0)), \n",
    "           c=mt.BLUE, s=50, alpha=0.7, edgecolors='white', linewidths=0.5, label='Class 0')\n",
    "ax.scatter(r_squared[y_circles==1], np.zeros(sum(y_circles==1)) + 0.1, \n",
    "           c=mt.RED, s=50, alpha=0.7, edgecolors='white', linewidths=0.5, label='Class 1')\n",
    "\n",
    "# Decision boundary\n",
    "threshold = 0.5\n",
    "ax.axvline(x=threshold, color=mt.GRAY, linestyle='--', linewidth=2, label='Decision boundary')\n",
    "\n",
    "ax.set_xlabel('$\\\\phi(x) = x_1^2 + x_2^2$')\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('Feature Space: Linearly Separable!', fontweight='bold', loc='left')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(-0.5, 0.6)\n",
    "ax.set_yticks([])\n",
    "mt.apply_economist_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation $\\phi$ maps data to a **feature space** where a linear classifier works.\n",
    "\n",
    "**Problem**: For complex patterns, we might need very high-dimensional (even infinite-dimensional) feature spaces. Computing $\\phi(x)$ explicitly becomes expensive or impossible.\n",
    "\n",
    "---\n",
    "## 4. The Kernel Trick\n",
    "\n",
    "The **kernel trick** is the key insight: many algorithms only need **dot products** between data points, not the features themselves.\n",
    "\n",
    "A **kernel function** computes the dot product in feature space directly:\n",
    "\n",
    "$$k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle$$\n",
    "\n",
    "We never compute $\\phi(x)$—we just use $k(x, x')$.\n",
    "\n",
    "### Common Kernels\n",
    "\n",
    "| Kernel | Formula | Feature Space |\n",
    "|--------|---------|---------------|\n",
    "| Linear | $k(x, x') = x^T x'$ | Original space |\n",
    "| Polynomial | $k(x, x') = (x^T x' + c)^d$ | All monomials up to degree $d$ |\n",
    "| RBF (Gaussian) | $k(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$ | Infinite-dimensional |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    \"\"\"Linear kernel: k(x, x') = x^T x'\"\"\"\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x1, x2, degree=3, c=1):\n",
    "    \"\"\"Polynomial kernel: k(x, x') = (x^T x' + c)^d\"\"\"\n",
    "    return (np.dot(x1, x2) + c) ** degree\n",
    "\n",
    "def rbf_kernel(x1, x2, gamma=1.0):\n",
    "    \"\"\"RBF (Gaussian) kernel: k(x, x') = exp(-gamma ||x - x'||^2)\"\"\"\n",
    "    return np.exp(-gamma * np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate kernel values\n",
    "x1 = np.array([1.0, 0.0])\n",
    "x2 = np.array([0.0, 1.0])\n",
    "x3 = np.array([1.0, 0.1])  # Close to x1\n",
    "\n",
    "print(\"Kernel values for sample points:\")\n",
    "print(f\"x1 = {x1}, x2 = {x2}, x3 = {x3}\")\n",
    "print()\n",
    "print(f\"Linear kernel:\")\n",
    "print(f\"  k(x1, x1) = {linear_kernel(x1, x1):.3f}\")\n",
    "print(f\"  k(x1, x2) = {linear_kernel(x1, x2):.3f}  (orthogonal)\")\n",
    "print(f\"  k(x1, x3) = {linear_kernel(x1, x3):.3f}  (similar)\")\n",
    "print()\n",
    "print(f\"RBF kernel (γ=1):\")\n",
    "print(f\"  k(x1, x1) = {rbf_kernel(x1, x1):.3f}  (identical)\")\n",
    "print(f\"  k(x1, x2) = {rbf_kernel(x1, x2):.3f}  (far apart)\")\n",
    "print(f\"  k(x1, x3) = {rbf_kernel(x1, x3):.3f}  (close)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel measures **similarity**: identical points have $k(x,x) = 1$, and similarity decays exponentially with distance.\n",
    "\n",
    "---\n",
    "## 5. Support Vector Machines with Kernels\n",
    "\n",
    "**Support Vector Machines (SVMs)** are the classic algorithm that exploits the kernel trick. The decision function only depends on dot products:\n",
    "\n",
    "$$f(x) = \\sum_{i \\in SV} \\alpha_i y_i k(x_i, x) + b$$\n",
    "\n",
    "where $SV$ is the set of **support vectors**—the training points closest to the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Compare kernels on the circles dataset\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "kernels = [\n",
    "    ('linear', {}),\n",
    "    ('poly', {'degree': 2}),\n",
    "    ('poly', {'degree': 3}),\n",
    "    ('rbf', {'gamma': 1.0}),\n",
    "]\n",
    "titles = ['Linear', 'Polynomial (d=2)', 'Polynomial (d=3)', 'RBF (γ=1)']\n",
    "\n",
    "for ax, (kernel, params), title in zip(axes, kernels, titles):\n",
    "    svm = SVC(kernel=kernel, **params)\n",
    "    svm.fit(X_circles, y_circles)\n",
    "    plot_decision_boundary(ax, svm, X_circles, y_circles, title)\n",
    "    ax.set_aspect('equal')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same comparison for XOR\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, (kernel, params), title in zip(axes, kernels, titles):\n",
    "    svm = SVC(kernel=kernel, **params)\n",
    "    svm.fit(X_xor, y_xor)\n",
    "    plot_decision_boundary(ax, svm, X_xor, y_xor, title)\n",
    "    ax.set_aspect('equal')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel handles both datasets well, creating flexible decision boundaries that wrap around the data.\n",
    "\n",
    "---\n",
    "## 6. Visualizing Support Vectors\n",
    "\n",
    "Support vectors are the critical training points that define the decision boundary. The SVM solution only depends on these points—other training data can be removed without changing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_with_support_vectors(X, y, kernel='rbf', gamma=1.0, C=1.0):\n",
    "    \"\"\"Plot SVM decision boundary with support vectors highlighted.\"\"\"\n",
    "    svm = SVC(kernel=kernel, gamma=gamma, C=C)\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    # Filled contours for classes\n",
    "    ax.contourf(xx, yy, Z > 0, alpha=0.3, cmap=ListedColormap([mt.BLUE, mt.RED]))\n",
    "    \n",
    "    # Decision boundary and margins\n",
    "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], colors=['gray', 'black', 'gray'],\n",
    "               linestyles=['--', '-', '--'], linewidths=[1, 2, 1])\n",
    "    \n",
    "    # All points\n",
    "    ax.scatter(X[y==0, 0], X[y==0, 1], c=mt.BLUE, s=50, alpha=0.5,\n",
    "               edgecolors='white', linewidths=0.5)\n",
    "    ax.scatter(X[y==1, 0], X[y==1, 1], c=mt.RED, s=50, alpha=0.5,\n",
    "               edgecolors='white', linewidths=0.5)\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    sv = svm.support_vectors_\n",
    "    ax.scatter(sv[:, 0], sv[:, 1], s=200, facecolors='none', \n",
    "               edgecolors=mt.GREEN, linewidths=2.5, label=f'Support vectors (n={len(sv)})')\n",
    "    \n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title(f'SVM with {kernel.upper()} Kernel\\nAccuracy: {svm.score(X, y):.1%}',\n",
    "                 fontweight='bold', loc='left')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return svm\n",
    "\n",
    "svm_circles = plot_svm_with_support_vectors(X_circles, y_circles, kernel='rbf', gamma=2.0)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total training points: {len(y_circles)}\")\n",
    "print(f\"Support vectors: {len(svm_circles.support_vectors_)} ({len(svm_circles.support_vectors_)/len(y_circles):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. The Effect of the RBF Parameter γ\n",
    "\n",
    "The RBF kernel has a parameter $\\gamma$ that controls how \"local\" the similarity measure is:\n",
    "\n",
    "- **Small γ**: Each point influences a large region → smoother boundary\n",
    "- **Large γ**: Each point only affects nearby points → more complex boundary\n",
    "\n",
    "This is the **bias-variance tradeoff** in kernel space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of gamma\n",
    "gammas = [0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, gamma in zip(axes, gammas):\n",
    "    svm = SVC(kernel='rbf', gamma=gamma)\n",
    "    svm.fit(X_circles, y_circles)\n",
    "    plot_decision_boundary(ax, svm, X_circles, y_circles, f'γ = {gamma}')\n",
    "    n_sv = len(svm.support_vectors_)\n",
    "    ax.set_title(f'γ = {gamma}\\nAcc: {svm.score(X_circles, y_circles):.1%}, SV: {n_sv}',\n",
    "                 fontweight='bold')\n",
    "    ax.set_aspect('equal')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- **γ = 0.1**: Underfitting—boundary is too smooth\n",
    "- **γ = 1.0**: Good fit—captures the circular structure\n",
    "- **γ = 10, 100**: Overfitting—boundary becomes jagged, wrapping tightly around individual points\n",
    "\n",
    "Note how the number of support vectors increases with γ—a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. The Regularization Parameter C\n",
    "\n",
    "SVMs also have a regularization parameter $C$:\n",
    "\n",
    "- **Small C**: More regularization → wider margins, more misclassifications allowed\n",
    "- **Large C**: Less regularization → narrower margins, tries to classify all training points correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noisier data to show C effect\n",
    "X_noisy, y_noisy = make_circles_data(200, noise=0.2, factor=0.5, seed=123)\n",
    "\n",
    "C_values = [0.01, 0.1, 1.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, C in zip(axes, C_values):\n",
    "    svm = SVC(kernel='rbf', gamma=1.0, C=C)\n",
    "    svm.fit(X_noisy, y_noisy)\n",
    "    plot_decision_boundary(ax, svm, X_noisy, y_noisy, f'C = {C}')\n",
    "    n_sv = len(svm.support_vectors_)\n",
    "    ax.set_title(f'C = {C}\\nAcc: {svm.score(X_noisy, y_noisy):.1%}, SV: {n_sv}',\n",
    "                 fontweight='bold')\n",
    "    ax.set_aspect('equal')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With noisy data:\n",
    "- **Small C**: Ignores some misclassified points for a smoother boundary\n",
    "- **Large C**: Tries to get every point right, potentially overfitting to noise\n",
    "\n",
    "---\n",
    "## 9. Kernel PCA: Beyond Classification\n",
    "\n",
    "Kernels aren't just for SVMs. **Kernel PCA** performs dimensionality reduction in the feature space, finding nonlinear structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Original data\n",
    "ax = axes[0]\n",
    "ax.scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], \n",
    "           c=mt.BLUE, s=40, alpha=0.7, edgecolors='white', linewidths=0.5)\n",
    "ax.scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], \n",
    "           c=mt.RED, s=40, alpha=0.7, edgecolors='white', linewidths=0.5)\n",
    "ax.set_title('Original Data', fontweight='bold', loc='left')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_aspect('equal')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('black')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Linear PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_circles)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.scatter(X_pca[y_circles==0, 0], X_pca[y_circles==0, 1], \n",
    "           c=mt.BLUE, s=40, alpha=0.7, edgecolors='white', linewidths=0.5)\n",
    "ax.scatter(X_pca[y_circles==1, 0], X_pca[y_circles==1, 1], \n",
    "           c=mt.RED, s=40, alpha=0.7, edgecolors='white', linewidths=0.5)\n",
    "ax.set_title('Linear PCA', fontweight='bold', loc='left')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_aspect('equal')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('black')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Kernel PCA\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=2.0)\n",
    "X_kpca = kpca.fit_transform(X_circles)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.scatter(X_kpca[y_circles==0, 0], X_kpca[y_circles==0, 1], \n",
    "           c=mt.BLUE, s=40, alpha=0.7, edgecolors='white', linewidths=0.5)\n",
    "ax.scatter(X_kpca[y_circles==1, 0], X_kpca[y_circles==1, 1], \n",
    "           c=mt.RED, s=40, alpha=0.7, edgecolors='white', linewidths=0.5)\n",
    "ax.set_title('Kernel PCA (RBF)', fontweight='bold', loc='left')\n",
    "ax.set_xlabel('KPC1')\n",
    "ax.set_ylabel('KPC2')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('black')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Linear PCA can't separate the circles.\")\n",
    "print(\"Kernel PCA projects to a space where they're separable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Choosing Hyperparameters with Cross-Validation\n",
    "\n",
    "Both γ and C must be tuned. Grid search with cross-validation is the standard approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Grid search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV accuracy: {grid_search.best_score_:.1%}\")\n",
    "print(f\"Test accuracy: {grid_search.score(X_test, y_test):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the grid search results\n",
    "results = grid_search.cv_results_\n",
    "scores = results['mean_test_score'].reshape(len(param_grid['C']), len(param_grid['gamma']))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "im = ax.imshow(scores, cmap='Greens', vmin=0.5, vmax=1.0)\n",
    "\n",
    "ax.set_xticks(range(len(param_grid['gamma'])))\n",
    "ax.set_yticks(range(len(param_grid['C'])))\n",
    "ax.set_xticklabels(param_grid['gamma'])\n",
    "ax.set_yticklabels(param_grid['C'])\n",
    "ax.set_xlabel('γ (gamma)', fontsize=12)\n",
    "ax.set_ylabel('C', fontsize=12)\n",
    "ax.set_title('Cross-Validation Accuracy', fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(param_grid['C'])):\n",
    "    for j in range(len(param_grid['gamma'])):\n",
    "        text = f'{scores[i, j]:.1%}'\n",
    "        color = 'white' if scores[i, j] > 0.8 else 'black'\n",
    "        ax.text(j, i, text, ha='center', va='center', fontsize=11, color=color)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**The kernel trick:**\n",
    "- Maps data to a higher-dimensional feature space where linear methods work\n",
    "- Computes dot products in feature space without explicit transformation\n",
    "- Enables flexible, nonlinear decision boundaries\n",
    "\n",
    "**Common kernels:**\n",
    "\n",
    "| Kernel | When to use |\n",
    "|--------|-------------|\n",
    "| Linear | When data is already linearly separable |\n",
    "| Polynomial | When polynomial features would help |\n",
    "| RBF | General-purpose; good default choice |\n",
    "\n",
    "**Key hyperparameters (for RBF SVM):**\n",
    "- **γ**: Controls locality of similarity. Small = smooth, large = complex\n",
    "- **C**: Regularization. Small = wider margin (more regularization), large = narrower margin\n",
    "\n",
    "**Beyond SVMs:**\n",
    "- Kernel PCA for nonlinear dimensionality reduction\n",
    "- Kernel regression, kernel k-means, and more\n",
    "\n",
    "**Historical note:** SVMs with kernels were state-of-the-art for many tasks before deep learning. They remain useful for smaller datasets and when interpretability matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploration\n",
    "\n",
    "Try modifying:\n",
    "- **Different datasets**: How does the RBF kernel handle the moons dataset (`sklearn.datasets.make_moons`)?\n",
    "- **Kernel comparison**: Which kernel works best for your own data?\n",
    "- **Feature scaling**: What happens if you don't standardize features before using RBF?\n",
    "- **Real data**: Apply kernel SVM to a real dataset like the Iris or Wine datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
